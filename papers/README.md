## Papers
### Unlearnable Examples
 - [Unlearnable Examples: Making Personal Data Unexploitable](https://arxiv.org/abs/2101.04898) -
 This paper attempts to make examples unlearnable by making them appear already learned to the given model. Rather than increase the loss, the algorithm decreases the loss by introducing adversarial noise. The researchers use Projected Gradient Descent, where they treat the data as a learnable parameter, train it, and then project the change back into a ball around *X*. They create 2 variations: one where each sample has its own individualized noise and one where each class has its own class-wise noise. Naturally, the class-wise is more efficient but less effective. The attack grants full knowledge of data and model to the attacker. 
 - [Robust Unlearnable Examples: Protecting Data Against Adversarial Learning](https://arxiv.org/abs/2203.14533) - 
 This paper addresses the vulnerability of the original algorithm used by Huang et al. to adversarial training, where the model is trained against adversarial examples. It is found that adversarial training can recover the original training data as it moves the data back in the direction of maximum loss. To make the attack resilient, they construct a min-min-max optimization problem, where they add another level of complexity in order to address adversarial training. Rather than minimizing the loss, they minimize the adverarial loss and they conclude that the "ball" around *X* for the defense should be larger than the ball for the attack, in order to ensure that the data is hidden. Furthermore, they account for transformations by optimizing for a whole host of transforms.
 - [Transferable Unlearnable Examples](https://arxiv.org/abs/2210.10114) -
 This paper creates transferable unlearnable examples that can transfer across data or training mechanisms effectively. They use the classwise separability discriminant (CSD) which maximizes the distance between noise generated for each different class. Using the CSD loss makes the different class perturbations form tight clusters far away from each other by having each perturbation gather around its own class's centroid and by maximizing the distance between centroids. This helps perturbations transfer by making linearly separable classes such that the perturbation can work on any other image in that class. The paper also reduces the contrastive learning loss so that unsupervised learning will only be able to extract the error minimizing noise into any kind of embedding or representation.
 ### Deepfake disruption 
 - [Disrupting Deepfakes: Adversarial Attacks Against Conditional Image Translation Networks and Facial Manipulation Systems](https://arxiv.org/pdf/2003.01279.pdf) -
 This paper makes three contributions. First, they pose a novel algorithm for disrupting image translation or deepfake models, where they create an adversarial attack which will work for different deepfake queries. They train the attack by minimizing the loss toward a target *t* for many different possible class queries in their Joint Class Transferable Disruption. Second, they create a system to defend against such attacks on GANs by introducing adversarial training for both the generator and discriminator; both models train adversarially. Third, they create an attack that can evade common blur defenses by optimizing their algorithm from part 1 against multiple different blurring algorithms.
 - [Disrupting Image-Translation-Based DeepFake Algorithms with Adversarial Attacks](https://openaccess.thecvf.com/content_WACVW_2020/papers/w4/Yeh_Disrupting_Image-Translation-Based_DeepFake_Algorithms_with_Adversarial_Attacks_WACVW_2020_paper.pdf) -
 This paper takes a different approach to disrupting deepfakes that is essentially making the deepfake unlearnable. They create two attacks, a nullifying attack and a distorting attack. The nullifying attack attempts to minimize the distance between *x'* and *y'* such that the deepfake model has no effect, and the distorting attack does the exact opposite. In particular, the nullifying attack is very similar to the concept of unlearnable examples. The attack works for a singular model on a singular piece of data and requires access to model gradients.
 - [Anti-Forgery: Towards a Stealthy and Robust DeepFake Disruption Attack via Adversarial Perceptual-aware Perturbations](https://www.ijcai.org/proceedings/2022/0107.pdf) -
 This paper improves upon past adversarial attacks against DeepFake models by making them more robust to input transformation as all previous methods could be corrupted by simple input reconstruction. Their main insight is to use the Lab color space, which has channels a, b, and L. This was motivated by how the Lab color space is more human oriented, so the attack would be more imperceptible, and how the Lab color space is perceptually uniform rather than sparse. Further, the RGB values in the ordinary color space are all correlated with each other which make perturbations easier to be corrupted after reconstruction. They use PGD to train the perturbations.
 - [MagDR: Mask-guided Detection and Reconstruction for Defending Deepfakes](https://arxiv.org/pdf/2103.14211.pdf) -
 This paper creates a defense against DeepFake disruption with MagDR using input reconstruction. They first identify if the input has been corrupted based on metrics to identifying successful outputs. If the input is found to be corrupted, then reconstruction begins. The first objective is to apply the masks: they generate masks based on a face parser and determine which masks to apply based on the objective given. Then, for each mask, a new image is created, and an ensemble of models work to reconstruct the input using each of these masked images. The algorithm outperforms past methods towards making the DeepFake models resilient.
 - [Feature Extraction Matters More: Universal Deepfake Disruption through Attacking Ensemble Feature Extractors](https://arxiv.org/pdf/2303.00200.pdf) -
 This paper uses gradient ensemble to create a universal attack to different data and DeepFake models. They first calculate the loss for all given models and for all given X. This loss is the MSE loss between feature extractor outputs, as most GAN's have one and they are a critical place to attack in the process, minus the Kullback-Leibler divergence between distributions of feature extractions as they wanted to reduce overfitting to just one or two easy to attack models. Then, after each iteration over deepfake model type, they use gradient ensemble, which is when they average the gradients of the loss (with respect to the final output rather than the feature extraction) together, and add the result to the perturbation.
 - [Initiative Defense against Facial Manipulation](https://ojs.aaai.org/index.php/AAAI/article/view/16254) -
 This paper creates an algorithm that creates perturbations using a surrogate model and a perturbation generation model. For each iteration in the dataset, first the surrogate model gives the output based on *X*, and then is updated based on the loss. Second, the generator model creates an perturbation based on *X* and is updated with a custom loss function. For example, for model-based manipulation in which model changes some attribute of the image, the loss contains an *L1* norm term, a cycle-consistency loss, and term designed to confuse the GANs discriminator. Experiments show increased efficacy over the results obtained by Ruiz et al.
 - [DeepFake Disrupter: The Detector of DeepFake Is My Friend](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_DeepFake_Disrupter_The_Detector_of_DeepFake_Is_My_Friend_CVPR_2022_paper.pdf) -
 This paper trains a generator to create adversarial perturbations to deepfake models. The main difference is they try to cooperate with a deepfake identification model, which can classify given deepfakes. They add two loss terms to the normal distance function. The first makes the discriminator classify the image as fake, and the second makes it classify the image as real. The combination of both of these is an attempt to create a balance. The perturbation can both be detected by a deepfake detector but it also isn't messy or visual enough to be detected by a human. Results show that the outputs are indeed more detectable as deepfakes.
 - [Restricted Black-box Adversarial Attack Against DeepFake Face Swapping](https://arxiv.org/pdf/2204.12347.pdf) - 
 This paper creates a restricted black box attack with access to neither data nor the model. They train a generator to attack an autoencoder, and then transfer the attack onto a random deepfake model. In order to enhance transferability, they apply a post regularization where they distill knowledge from the attack to a new attack, which reduces specificity. Results show significantly less distortion than other attacks, but of course this is a more restricted attack.
 - [CMUA-Watermark: A Cross-Model Universal Adversarial Watermark for Combating Deepfakes](https://arxiv.org/pdf/2105.10872.pdf)
 This paper extends the approach taken by Ruiz et al. 2020 to work for all images and all models. They train the perturbation on a batch of images using PGD, and then they average the output perturbation with all the other perturbations for the specific targeted model. Then, for each model, they take a weighted average of the current perturbation and the newly generated perturbation for the model according to decay factor alpha. They use the Tree-structured Parzen Estimator to decide a multiplicative factor for the perturbation on each given model, since perturbation performance relies greatly on the norm of the vector.
 ### DeepFake models
 - StarGAN: [StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation](https://openaccess.thecvf.com/content_cvpr_2018/papers/Choi_StarGAN_Unified_Generative_CVPR_2018_paper.pdf) -
 StarGAN uses a single generator to transfer from class to class. Given an image and a target label, the generator outputs a new image with the given label. Adversarial, classification, and cycle consistencies losses are used to attain viable generated images. StarGAN further uses a mask for use with multiple datasets; since each dataset has a different set of labels, a mask allows for some labels to no be accounted for. This feature allows StarGAN to work for many different datasets simultaneously. 
 - AGGAN: [Attention-Guided Generative Adversarial Networks for Unsupervised Image-to-Image Translation](https://arxiv.org/abs/1903.12296) - 
 The main idea of AGGAN is to use attention to prevent the generators from modifiying parts of the face that do not need to be modified. The generator creates a content mask and an attention mask given input *X*, and for all pixels that are unmasked, the content mask is used, and for all that are masked, the input data is used. They have a different adversarial loss, where the discriminator receives the mask as input, a cycle-consistency loss, an *L1* loss between the generated image and input image to minimize changes, and an attention loss to ensure the masks do not become oversaturated with ones.
 - AttGAN: [AttGAN: Facial Attribute Editing by Only Changing What You Want](https://arxiv.org/abs/1711.10678) - 
 AttGAN uses an encoder-decoder GAN architecture which first generates a latent space representation of the input image and then uses the decoder to, given the representation and the desired output class, generate the finished deepfake. They use an adversarial loss, a classification loss, and a reconstruction loss resembling cycle-consistency. They also add functionality to change the style of an image by binding style controllers to certain attributes, therefore allowing style controllers to be inputted to change the style of the image.
 - HiSD: [Image-to-image Translation via Hierarchical Style Disentanglement](https://arxiv.org/abs/2103.01456) -
 This paper creates hierarchical way of modeling image translation labels, where rather than model labels as simple attributes, HiSD models them as variations of attributes; for example, the different hair colors are all clustered together rather than placed disparately. HiSD uses an encoder-decoder structure as well, similar to AttGAN, except it conducts translation directly on the latent representation rather than in the decoder.
 - StarGAN v2: [StarGAN v2: Diverse Image Synthesis for Multiple Domains](https://arxiv.org/abs/1912.01865) - 
 The goal of this paper is to be able to create diverse translated images across many different domains. Four main modules are created: first, a generator that can take in an image and a style and output the translated image; second, a model that can take in a latent represenation and output styles for each domain; third, a model that takes in a image and does the same as the previous; fourth, a discriminator to tell if images are real or fake. The idea is to be able to inject many different styles from a single domain, such as to be able to inject all of another image's style into an image, rather than have to work one style at a time.
 ### Misc.
 - [Adversarial Examples Are Not Bugs, They Are Features](https://proceedings.neurips.cc/paper/2019/hash/e2c420d928d4bf8ce0ff2ec19b371514-Abstract.html) - 
 First, a distinction is made between non-robust and robust features, where robust features are those that are salient to the human eye and non-robust features are those that may only appear to a ANNs eyes. Robust features are defined as features that have a correlation with a class which remains with added noise whereas non-robust features have a correlation with a class which is sensitive to noise. The critical finding is that models can train on non-robust datasets (as in datasets that have been adversarially attacked) and still reach high accuracy on the real dataset. This would suggest that vulnerability to adversarial attacks does not necessarily come from the model, but from underlying, invisible, and undetectable patterns in the dataset which the model learns. They also find that creating a robustified version of the dataset where all features are both salient and have correlation with a class yields robust accuracy which provides further evidence that vulnerability is a property of the dataset. The researchers support their hypothesis with rigorous, mathematical proofs.
 
 #### Not done yet
 

 
https://arxiv.org/abs/2105.10872
https://arxiv.org/pdf/2204.12347.pdf



