## Papers
### Unlearnable Examples
 - [Unlearnable Examples: Making Personal Data Unexploitable](https://arxiv.org/abs/2101.04898) -
 This paper attempts to make examples unlearnable by making them appear already learned to the given model. Rather than increase the loss, the algorithm decreases the loss by introducing adversarial noise. The researchers use Projected Gradient Descent, where they treat the data as a learnable parameter, train it, and then project the change back into a ball around *X*. They create 2 variations: one where each sample has its own individualized noise and one where each class has its own class-wise noise. Naturally, the class-wise is more efficient but less effective. The attack grants full knowledge of data and model to the attacker. 
 - [Robust Unlearnable Examples: Protecting Data Against Adversarial Learning](https://arxiv.org/abs/2203.14533) - 
 This paper addresses the vulnerability of the original algorithm used by Huang et al. to adversarial training, where the model is trained against adversarial examples. It is found that adversarial training can recover the original training data as it moves the data back in the direction of maximum loss. To make the attack resilient, they construct a min-min-max optimization problem, where they add another level of complexity in order to address adversarial training. Rather than minimizing the loss, they minimize the adverarial loss and they conclude that the "ball" around *X* for the defense should be larger than the ball for the attack, in order to ensure that the data is hidden. Furthermore, they account for transformations by optimizing for a whole host of transforms.
 - [Transferable Unlearnable Examples](https://arxiv.org/abs/2210.10114) -
 This paper creates transferable unlearnable examples that can transfer across data or training mechanisms effectively. They use the classwise separability discriminant (CSD) which maximizes the distance between noise generated for each different class. Using the CSD loss makes the different class perturbations form tight clusters far away from each other by having each perturbation gather around its own class's centroid and by maximizing the distance between centroids. This helps perturbations transfer by making linearly separable classes such that the perturbation can work on any other image in that class. The paper also reduces the contrastive learning loss so that unsupervised learning will only be able to extract the error minimizing noise into any kind of embedding or representation.
 ### Deepfake disruption
 - [Disrupting Deepfakes: Adversarial Attacks Against Conditional Image Translation Networks and Facial Manipulation Systems](https://arxiv.org/pdf/2003.01279.pdf)
 This paper makes three contributions. First, they pose a novel algorithm for disrupting image translation or deepfake models, where they create an adversarial attack which will work for different deepfake queries. They train the attack by minimizing the loss toward a target *t* for many different possible class queries in their Joint Class Transferable Disruption. Second, they create a system to defend against such attacks on GANs by introducing adversarial training for both the generator and discriminator; both models train adversarially. Third, they create an attack that can evade common blur defenses by optimizing their algorithm from part 1 against multiple different blurring algorithms.
 - [Anti-Forgery: Towards a Stealthy and Robust DeepFake Disruption Attack via Adversarial Perceptual-aware Perturbations](https://www.ijcai.org/proceedings/2022/0107.pdf)
 This paper improves upon past adversarial attacks against DeepFake models by making them more robust to input transformation as all previous methods could be corrupted by simple input reconstruction. Their main insight is to use the Lab color space, which has channels a, b, and L. This was motivated by how the Lab color space is more human oriented, so the attack would be more imperceptible, and how the Lab color space is perceptually uniform rather than sparse. Further, the RGB values in the ordinary color space are all correlated with each other which make perturbations easier to be corrupted after reconstruction. They use PGD to train the perturbations.
 - [MagDR: Mask-guided Detection and Reconstruction for Defending Deepfakes](https://arxiv.org/pdf/2103.14211.pdf)
 This paper creates a defense against DeepFake disruption with MagDR using input reconstruction. They first identify if the input has been corrupted based on metrics to identifying successful outputs. If the input is found to be corrupted, then reconstruction begins. The first objective is to apply the masks: they generate masks based on a face parser and determine which masks to apply based on the objective given. Then, for each mask, a new image is created, and an ensemble of models work to reconstruct the input using each of these masked images. The algorithm outperforms past methods towards making the DeepFake models resilient.
 ### DeepFake models
 ### Misc.
 - [Adversarial Examples Are Not Bugs, They Are Features](https://proceedings.neurips.cc/paper/2019/hash/e2c420d928d4bf8ce0ff2ec19b371514-Abstract.html) - 
 First, a distinction is made between non-robust and robust features, where robust features are those that are salient to the human eye and non-robust features are those that may only appear to a ANNs eyes. Robust features are defined as features that have a correlation with a class which remains with added noise whereas non-robust features have a correlation with a class which is sensitive to noise. The critical finding is that models can train on non-robust datasets (as in datasets that have been adversarially attacked) and still reach high accuracy on the real dataset. This would suggest that vulnerability to adversarial attacks does not necessarily come from the model, but from underlying, invisible, and undetectable patterns in the dataset which the model learns. They also find that creating a robustified version of the dataset where all features are both salient and have correlation with a class yields robust accuracy which provides further evidence that vulnerability is a property of the dataset. The researchers support their hypothesis with rigorous, mathematical proofs.
 #### Not done yet
 https://arxiv.org/pdf/2303.00200.pdf
 https://www.proquest.com/docview/2677484353?pq-origsite=gscholar&fromopenview=true
 https://arxiv.org/pdf/2006.07421.pdf
 https://openaccess.thecvf.com/content_WACVW_2020/papers/w4/Yeh_Disrupting_Image-Translation-Based_DeepFake_Algorithms_with_Adversarial_Attacks_WACVW_2020_paper.pdf



