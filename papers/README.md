## Papers

 - [Adversarial Examples Are Not Bugs, They Are Features](https://proceedings.neurips.cc/paper/2019/hash/e2c420d928d4bf8ce0ff2ec19b371514-Abstract.html) - 
 First, a distinction is made between non-robust and robust features, where robust features are those that are salient to the human eye and non-robust features are those that may only appear to a ANNs' eyes. Robust features are defined as features that have a correlation with a class which remains with added noise whereas non-robust features have a correlation with a class which is sensitive to noise. The critical finding is that models can train on non-robust datasets (as in datasets that have been adversarially attacked) and still reach high accuracy on the real dataset. This would suggest that vulnerability to adversarial attacks does not necessarily come from the model, but from underlying, invisible, and undetectable patterns in the dataset which the model learns. They also find that creating a robustified version of the dataset where all features are both salient and have correlation with a class yields robust accuracy which provides further evidence that vulnerability is a property of the dataset. The researchers support their hypothesis with rigorous, mathematical proofs.
 - [Unlearnable Examples: Making Personal Data Unexploitable](https://arxiv.org/abs/2101.04898) -
 This paper attempts to make examples unlearnable by making them appear already learned to the given model. Rather than increase the loss, the algorithm decreases the loss by introducing adversarial noise. The researchers use Projected Gradient Descent, where they treat the data as a learnable parameter, train it, and then project the change back into a ball around *X*. They create 2 variations: one where each sample has its own individualized noise and one where each class has its own class-wise noise. Naturally, the class-wise is more efficient but less effective. The attack grants full knowledge of data and model to the attacker. 
 - [Robust Unlearnable Examples: Protecting Data Against Adversarial Learning](https://arxiv.org/abs/2203.14533) - 
 This paper addresses the vulnerability of the original algorithm used by Huang et al. to adversarial training, where the model is trained against adversarial examples. It is found that adversarial training can recover the original training data as it moves the data back in the direction of maximum loss. To make the attack resilient, they construct a min-min-max optimization problem, where they add another level of complexity in order to address adversarial training. Rather than minimizing the loss, they minimize the adverarial loss and they conclude that the "ball" around *X* for the defense should be larger than the ball for the attack, in order to ensure that the data is hidden. Furthermore, they account for transformations by optimizing for a whole host of transforms.
 - [Transferable Unlearnable Examples](https://arxiv.org/abs/2210.10114) -
 This paper creates transferable unlearnable examples that can transfer across data or training mechanisms effectively. They use the classwise separability discriminant (CSD) which maximizes the distance between noise generated for each different class. Using the CSD loss makes the different class perturbations form tight clusters far away from each other by having each perturbation gather around its own class's centroid and by maximizing the distance between centroids. This helps perturbations transfer by making linearly separable classes such that the perturbation can work on any other image in that class. The paper also reduces the contrastive learning loss so that unsupervised learning will only be able to extract the error minimizing noise into any kind of embedding or representation.

